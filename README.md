# ğŸ“Š Projet de Trading GBP/USD avec ML et RL

SystÃ¨me de trading algorithmique complet pour la paire GBP/USD utilisant Machine Learning et Reinforcement Learning.

---

## ğŸ¯ Objectif

DÃ©velopper un systÃ¨me de trading automatisÃ© capable de :
- Analyser les donnÃ©es de marchÃ© GBP/USD (timeframe 15 minutes)
- GÃ©nÃ©rer des signaux de trading avec ML et RL
- Backtester les stratÃ©gies
- Optimiser les performances

---

## ğŸ† RÃ©sultats

### Meilleure Performance : **+297% de return** (Logistic Regression - T07)

| ModÃ¨le | Return | Trades | Win Rate | AnnÃ©e Test |
|--------|--------|--------|----------|------------|
| **Logistic Regression** | **+297.54%** | 10 | 40% | 2024 |
| Random Forest | 0% | 0 | - | 2024 |
| XGBoost | -0.14% | 2 | 0% | 2024 |
| DQN Agent (RL) | Variable | Variable | ~50% | 2024 |

---

## ğŸ“‚ Structure du Projet

```
Projet/
â”œâ”€â”€ api/                      # API FastAPI
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ data.py          # T01-T04 : DonnÃ©es
â”‚   â”‚   â”œâ”€â”€ features.py      # T05 : Features techniques
â”‚   â”‚   â””â”€â”€ strategies.py    # T06 : StratÃ©gies baseline
â”‚   â”œâ”€â”€ services/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/              # T07 : Machine Learning
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â”œâ”€â”€ ml_trainer.py
â”‚   â”‚   â”œâ”€â”€ ml_backtester.py
â”‚   â”‚   â””â”€â”€ run_ml_pipeline.py
â”‚   â””â”€â”€ rl/                  # T08 : Reinforcement Learning
â”‚       â”œâ”€â”€ trading_env.py
â”‚       â”œâ”€â”€ dqn_agent.py
â”‚       â”œâ”€â”€ train_rl.py
â”‚       â””â”€â”€ rl_backtester.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # DonnÃ©es brutes (.csv)
â”‚   â””â”€â”€ processed/           # DonnÃ©es traitÃ©es (.parquet)
â”œâ”€â”€ models/saved/            # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ ml/                  # ModÃ¨les ML
â”‚   â””â”€â”€ rl/                  # Agents RL
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_pipeline.ipynb
â”‚   â”œâ”€â”€ 02_ml_models.ipynb
â”‚   â””â”€â”€ 03_reinforcement_learning.ipynb
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â””â”€â”€ README.md               # Ce fichier
```

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- pip
- Git

### Installation rapide

```bash
# Cloner le repository
git clone <votre-repo>
cd Projet

# Installer les dÃ©pendances
pip install -r requirements.txt

# VÃ©rifier l'installation
python -c "import pandas, numpy, sklearn, torch; print('âœ“ Installation rÃ©ussie')"
```

---

## ğŸ“– Utilisation

### 1ï¸âƒ£ API FastAPI (T05-T06)

```bash
# DÃ©marrer l'API
uvicorn api.main:app --reload

# AccÃ©der Ã  la documentation
# http://localhost:8000/docs
```

**Endpoints disponibles** :
- `POST /features/compute/{year}` - Calculer les features techniques
- `POST /strategies/backtest/{strategy}` - Backtester une stratÃ©gie

### 2ï¸âƒ£ Machine Learning (T07)

```bash
# Pipeline complet (crÃ©ation datasets + entraÃ®nement + backtesting)
python src/models/run_ml_pipeline.py

# Fichiers gÃ©nÃ©rÃ©s :
# - data/processed/ml_dataset_*.parquet
# - models/saved/*.pkl
```

**RÃ©sultats attendus** :
- Logistic Regression : +297% return sur 2024
- Random Forest : 0% (trop conservateur)
- XGBoost : -0.14% (peu actif)

### 3ï¸âƒ£ Reinforcement Learning (T08)

```bash
# EntraÃ®ner l'agent DQN (20 Ã©pisodes pour test rapide)
python src/rl/train_rl.py

# Backtester l'agent entraÃ®nÃ©
python src/rl/rl_backtester.py
```

**Configuration** :
- Environnement : Gym custom
- Agent : DQN (Deep Q-Network)
- Actions : HOLD, BUY, SELL
- Training : 2022, Validation : 2023, Test : 2024

### 4ï¸âƒ£ Notebooks Jupyter

```bash
# Lancer Jupyter
jupyter notebook

# Ouvrir les notebooks dans notebooks/
```

**Notebooks disponibles** :
- `01_data_pipeline.ipynb` - Exploration des donnÃ©es (T01-T04)
- `02_ml_models.ipynb` - Machine Learning (T07)
- `03_reinforcement_learning.ipynb` - RL Analysis (T08)

---

## ğŸ“Š Pipeline de DonnÃ©es

### T01-T04 : Data Pipeline

1. **T01** : Chargement donnÃ©es brutes (CSV)
2. **T02** : AgrÃ©gation en bougies M15
3. **T03** : Nettoyage et validation
4. **T04** : ContrÃ´le qualitÃ©

**DonnÃ©es disponibles** : 2022, 2023, 2024 (format M15 - 15 minutes)

### T05 : Feature Engineering

**20+ indicateurs techniques** calculÃ©s via l'API :
- Prix : returns, volatility, body, wicks
- Trend : EMA (20, 50, 200), slope
- Momentum : RSI, MACD, ADX
- Volatility : ATR, rolling std

### T06 : Baseline Strategies

**3 stratÃ©gies de rÃ©fÃ©rence** :
- Buy & Hold : -10.21% (2022)
- Random Trading : -7.99% (2022)
- SMA Crossover : -5.67% (2022)

---

## ğŸ¤– Machine Learning (T07)

### Approche

- **Target** : Classification 3 classes (UP, HOLD, DOWN)
- **Threshold** : 0.1% (10 pips)
- **Features** : 100+ features (lag, rolling, technical indicators)
- **Train** : 2022-2023
- **Test** : 2024

### ModÃ¨les

1. **Logistic Regression** â­
   - Return : +297.54%
   - Trades : 10
   - Win Rate : 40%
   - **Meilleur modÃ¨le !**

2. **Random Forest**
   - Return : 0%
   - Trades : 0
   - Trop conservateur

3. **XGBoost**
   - Return : -0.14%
   - Trades : 2
   - Peu actif

### Fichiers gÃ©nÃ©rÃ©s

```
models/saved/
â”œâ”€â”€ 2022_2023_logistic_regression_*.pkl
â”œâ”€â”€ 2022_2023_random_forest_*.pkl
â”œâ”€â”€ 2022_2023_xgboost_*.pkl
â”œâ”€â”€ 2022_2023_scaler_main_*.pkl
â”œâ”€â”€ 2022_2023_feature_names_*.pkl
â””â”€â”€ 2022_2023_metrics_*.pkl
```

---

## ğŸ® Reinforcement Learning (T08)

### Agent DQN

**Architecture** :
```
State (features + position) 
  â†“
Dense(128) + ReLU + Dropout(0.2)
  â†“
Dense(64) + ReLU + Dropout(0.2)
  â†“
Output(3) â†’ Q-values [HOLD, BUY, SELL]
```

**HyperparamÃ¨tres** :
- Learning rate : 0.001
- Gamma : 0.99
- Epsilon decay : 0.995
- Buffer size : 10,000
- Batch size : 64

### EntraÃ®nement

- **Train** : 2022 (24,814 pÃ©riodes)
- **Validation** : 2023 (21,450 pÃ©riodes)
- **Test** : 2024 (24,831 pÃ©riodes)
- **Ã‰pisodes** : 20-200 (configurable)

### RÃ©sultats

Performance variable selon hyperparamÃ¨tres et reward function.
L'agent DQN nÃ©cessite optimisation pour battre les modÃ¨les ML.

---

## ğŸ“ˆ Comparaison des Approches

| CritÃ¨re | ML (T07) | RL (T08) |
|---------|----------|----------|
| **Meilleur Return** | +297% (LogReg) | Variable |
| **Trades** | 10 (sÃ©lectif) | Variable |
| **Temps training** | ~5 min | ~30 min |
| **ComplexitÃ©** | Moyenne | Ã‰levÃ©e |
| **StabilitÃ©** | âœ… Stable | âš ï¸ Variable |
| **AdaptabilitÃ©** | âŒ Statique | âœ… Apprend en continu |

**Recommandation actuelle** : Logistic Regression (T07) pour production

---

## ğŸ”§ Configuration

### Variables d'environnement (optionnel)

CrÃ©er un fichier `.env` :

```env
# API
API_HOST=0.0.0.0
API_PORT=8000

# DonnÃ©es
DATA_PATH=data/processed

# ModÃ¨les
MODELS_PATH=models/saved
```

### Fichiers de donnÃ©es requis

```
data/processed/
â”œâ”€â”€ m15_2022.csv
â”œâ”€â”€ m15_2023.csv
â””â”€â”€ m15_2024.csv
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨mes courants

**1. Import Error : "No module named 'gymnasium'"**
```bash
pip install gymnasium torch
```

**2. PyArrow Error (lecture parquet)**
```bash
pip install pyarrow
```

**3. API ne dÃ©marre pas**
```bash
# VÃ©rifier que vous Ãªtes Ã  la racine du projet
cd Projet
uvicorn api.main:app --reload
```

**4. CUDA Out of Memory (RL)**
```bash
# Utiliser CPU au lieu de GPU
# Dans train_rl.py, ligne 60 :
device='cpu'
```

---

## ğŸ“š Documentation DÃ©taillÃ©e

Chaque tÃ¢che (T01-T08) possÃ¨de sa propre documentation :

- **T01-T06** : Voir documentation API (`/docs`)
- **T07** : Voir `README_T07.md` (Ã  crÃ©er si besoin)
- **T08** : Voir `README_T08.md`

---

## ğŸ“ Ressources

### Articles de rÃ©fÃ©rence

- **DQN** : [Mnih et al., 2015](https://www.nature.com/articles/nature14236)
- **Feature Engineering** : Indicateurs techniques standards

### Technologies utilisÃ©es

- **Backend** : FastAPI, Pandas, NumPy
- **ML** : scikit-learn, XGBoost
- **RL** : PyTorch, Gymnasium
- **Visualisation** : Matplotlib, Seaborn
- **Data** : Parquet, CSV

---

## ğŸ¤ Contribution

Ce projet est un projet Ã©ducatif de trading algorithmique.

### AmÃ©liorations possibles

- [ ] Agents RL avancÃ©s (PPO, A2C)
- [ ] Optimisation hyperparamÃ¨tres (Grid Search)
- [ ] Feature selection automatique
- [ ] Dashboard Streamlit
- [ ] DÃ©ploiement Docker
- [ ] Trading multi-actifs
- [ ] Gestion de portefeuille

---

## âš ï¸ Disclaimer

**Ce projet est Ã  but Ã©ducatif uniquement.**

- âŒ Ne constitue PAS un conseil financier
- âŒ Trading rÃ©el Ã  vos risques et pÃ©rils
- âŒ Performances passÃ©es ne garantissent pas les futures
- âœ… Utilisez un compte dÃ©mo pour tester

---

## ğŸ“ Licence

Projet Ã©ducatif - Tous droits rÃ©servÃ©s

---

## ğŸ“§ Contact

Pour toute question sur le projet, veuillez crÃ©er une issue sur GitHub.

---

## âœ… Statut du Projet

- [x] T01 - Chargement donnÃ©es
- [x] T02 - AgrÃ©gation M15
- [x] T03 - Nettoyage
- [x] T04 - QualitÃ©
- [x] T05 - Features API
- [x] T06 - Baseline strategies
- [x] T07 - Machine Learning (+297% !)
- [x] T08 - Reinforcement Learning
- [x] T09 - Production & Documentation

**Projet complÃ©tÃ© !** ğŸ‰
