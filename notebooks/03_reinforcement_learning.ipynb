{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook 3 : Reinforcement Learning (T08)\n",
    "\n",
    "Ce notebook explore l'agent de trading bas√© sur le Reinforcement Learning :\n",
    "- **Environnement Gym** : Custom trading environment\n",
    "- **Agent DQN** : Deep Q-Network avec experience replay\n",
    "- **Entra√Ænement** : 2022 (train) + 2023 (validation)\n",
    "- **Test** : Performance sur 2024\n",
    "- **Comparaison** : DQN vs ML (T07)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Ajouter le chemin du projet\n",
    "sys.path.append('../src/rl')\n",
    "\n",
    "from trading_env import TradingEnv\n",
    "from dqn_agent import DQNAgent\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Environnement de Trading\n",
    "\n",
    "### Espace d'actions\n",
    "- **0** : HOLD (ne rien faire)\n",
    "- **1** : BUY (position longue)\n",
    "- **2** : SELL (position courte / fermer position)\n",
    "\n",
    "### Espace d'√©tats\n",
    "- Features techniques (RSI, MACD, EMAs, etc.)\n",
    "- Position actuelle\n",
    "- P&L non r√©alis√©\n",
    "- Balance normalis√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es\n",
    "df_2022 = pd.read_parquet('../data/processed/ml_dataset_2022.parquet')\n",
    "\n",
    "print(f\" Donn√©es charg√©es (2022)\")\n",
    "print(f\"   Lignes: {len(df_2022):,}\")\n",
    "print(f\"   Colonnes: {len(df_2022.columns)}\")\n",
    "\n",
    "df_2022.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'environnement\n",
    "env = TradingEnv(df_2022, initial_balance=10000)\n",
    "\n",
    "print(\" Environnement cr√©√©\")\n",
    "print(f\"   State size: {env.observation_space.shape[0]}\")\n",
    "print(f\"   Action size: {env.action_space.n}\")\n",
    "print(f\"   Feature columns: {len(env.feature_cols)}\")\n",
    "print(f\"\\nüìã Premi√®res features:\")\n",
    "print(env.feature_cols[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test de l'environnement\n",
    "\n",
    "Simulons quelques actions pour comprendre le fonctionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environnement\n",
    "state, info = env.reset()\n",
    "\n",
    "print(\" Environnement reset√©\")\n",
    "print(f\"   State shape: {state.shape}\")\n",
    "print(f\"   Balance initiale: {info['balance']:.2f}\")\n",
    "\n",
    "# Faire quelques actions al√©atoires\n",
    "actions_log = []\n",
    "rewards_log = []\n",
    "\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()  # Action al√©atoire\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    actions_log.append(action)\n",
    "    rewards_log.append(reward)\n",
    "    \n",
    "    action_name = ['HOLD', 'BUY', 'SELL'][action]\n",
    "    print(f\"Step {i+1}: Action={action_name}, Reward={reward:.4f}, Balance={info['balance']:.2f}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analyse de l'entra√Ænement\n",
    "\n",
    "Chargeons l'historique d'entra√Ænement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger l'historique d'entra√Ænement\n",
    "history_file = Path('../models/saved/rl/training_history.json')\n",
    "\n",
    "if history_file.exists():\n",
    "    with open(history_file, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    train_hist = history['train']\n",
    "    val_hist = history['val']\n",
    "    \n",
    "    print(\"‚úì Historique charg√©\")\n",
    "    print(f\"   √âpisodes d'entra√Ænement: {len(train_hist['episodes'])}\")\n",
    "    print(f\"   √âpisodes de validation: {len(val_hist['episodes'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Fichier d'historique non trouv√©\")\n",
    "    print(\"   Assurez-vous d'avoir entra√Æn√© l'agent avec train_rl.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### √âvolution des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er DataFrame pour faciliter l'analyse\n",
    "train_df = pd.DataFrame({\n",
    "    'episode': train_hist['episodes'],\n",
    "    'return': train_hist['returns'],\n",
    "    'balance': train_hist['balances'],\n",
    "    'trades': train_hist['trades'],\n",
    "    'win_rate': train_hist['win_rates'],\n",
    "    'loss': train_hist['losses'],\n",
    "    'epsilon': train_hist['epsilons']\n",
    "})\n",
    "\n",
    "print(\" Statistiques d'entra√Ænement:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation compl√®te\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Returns\n",
    "axes[0, 0].plot(train_df['episode'], train_df['return'], label='Train', marker='o', alpha=0.7)\n",
    "if val_hist['episodes']:\n",
    "    axes[0, 0].plot(val_hist['episodes'], val_hist['returns'], \n",
    "                   label='Validation', marker='s', alpha=0.7, color='orange')\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].set_title('Total Return (%)', fontsize=14)\n",
    "axes[0, 0].set_xlabel('√âpisode')\n",
    "axes[0, 0].set_ylabel('Return %')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Balance\n",
    "axes[0, 1].plot(train_df['episode'], train_df['balance'], label='Train', marker='o', alpha=0.7)\n",
    "if val_hist['episodes']:\n",
    "    axes[0, 1].plot(val_hist['episodes'], val_hist['balances'], \n",
    "                   label='Validation', marker='s', alpha=0.7, color='orange')\n",
    "axes[0, 1].axhline(y=10000, color='black', linestyle='--', alpha=0.3, label='Initial')\n",
    "axes[0, 1].set_title('Balance Finale', fontsize=14)\n",
    "axes[0, 1].set_xlabel('√âpisode')\n",
    "axes[0, 1].set_ylabel('Balance ($)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Nombre de trades\n",
    "axes[1, 0].plot(train_df['episode'], train_df['trades'], marker='o', alpha=0.7, color='green')\n",
    "axes[1, 0].set_title('Nombre de Trades', fontsize=14)\n",
    "axes[1, 0].set_xlabel('√âpisode')\n",
    "axes[1, 0].set_ylabel('Trades')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Win Rate\n",
    "axes[1, 1].plot(train_df['episode'], train_df['win_rate'], label='Train', marker='o', alpha=0.7)\n",
    "if val_hist['episodes']:\n",
    "    axes[1, 1].plot(val_hist['episodes'], val_hist['win_rates'], \n",
    "                   label='Validation', marker='s', alpha=0.7, color='orange')\n",
    "axes[1, 1].axhline(y=0.5, color='black', linestyle='--', alpha=0.3, label='50%')\n",
    "axes[1, 1].set_title('Win Rate', fontsize=14)\n",
    "axes[1, 1].set_xlabel('√âpisode')\n",
    "axes[1, 1].set_ylabel('Win Rate')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Epsilon (exploration)\n",
    "axes[2, 0].plot(train_df['episode'], train_df['epsilon'], marker='o', alpha=0.7, color='purple')\n",
    "axes[2, 0].set_title('Taux d\\'Exploration (Epsilon)', fontsize=14)\n",
    "axes[2, 0].set_xlabel('√âpisode')\n",
    "axes[2, 0].set_ylabel('Epsilon')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Loss\n",
    "axes[2, 1].plot(train_df['episode'], train_df['loss'], marker='o', alpha=0.7, color='red')\n",
    "axes[2, 1].set_title('Loss (erreur d\\'apprentissage)', fontsize=14)\n",
    "axes[2, 1].set_xlabel('√âpisode')\n",
    "axes[2, 1].set_ylabel('Loss')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analyse d√©taill√©e\n",
    "\n",
    "### Probl√®mes potentiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des probl√®mes\n",
    "print(\" DIAGNOSTIC\\n\")\n",
    "\n",
    "# 1. Return n√©gatif ?\n",
    "avg_return = train_df['return'].mean()\n",
    "if avg_return < 0:\n",
    "    print(f\"‚ùå PROBL√àME: Return moyen n√©gatif ({avg_return:.2f}%)\")\n",
    "    print(\"   ‚Üí L'agent perd de l'argent en moyenne\")\n",
    "else:\n",
    "    print(f\"‚úÖ Return moyen positif: {avg_return:.2f}%\")\n",
    "\n",
    "# 2. Trop de trades ?\n",
    "avg_trades = train_df['trades'].mean()\n",
    "total_steps = len(df_2022) - env.lookback_window\n",
    "trade_ratio = avg_trades / total_steps\n",
    "\n",
    "print(f\"\\n Activit√© de trading:\")\n",
    "print(f\"   Trades moyens: {avg_trades:.0f}\")\n",
    "print(f\"   Ratio: {trade_ratio:.2%} (trades / p√©riodes)\")\n",
    "\n",
    "if trade_ratio > 0.3:\n",
    "    print(\"    L'agent trade BEAUCOUP (>30% du temps)\")\n",
    "    print(\"   ‚Üí Les frais de transaction mangent les profits\")\n",
    "    print(\"   ‚Üí Solution: Augmenter transaction_cost ou p√©naliser les trades\")\n",
    "elif trade_ratio < 0.05:\n",
    "    print(\"    L'agent trade TR√àS PEU (<5% du temps)\")\n",
    "    print(\"   ‚Üí Peut-√™tre trop conservateur\")\n",
    "else:\n",
    "    print(\"    Bon niveau de trading\")\n",
    "\n",
    "# 3. Win rate\n",
    "avg_win_rate = train_df['win_rate'].mean()\n",
    "print(f\"\\n Win Rate: {avg_win_rate:.2%}\")\n",
    "if avg_win_rate < 0.45:\n",
    "    print(\"    Win rate trop faible (<45%)\")\n",
    "elif avg_win_rate > 0.55:\n",
    "    print(\"    Bon win rate (>55%)\")\n",
    "else:\n",
    "    print(\"    Win rate moyen (proche de 50%)\")\n",
    "\n",
    "# 4. Apprentissage\n",
    "if len(train_df) > 5:\n",
    "    first_5 = train_df.head(5)['return'].mean()\n",
    "    last_5 = train_df.tail(5)['return'].mean()\n",
    "    improvement = last_5 - first_5\n",
    "    \n",
    "    print(f\"\\n Progression:\")\n",
    "    print(f\"   Return premiers 5 √©pisodes: {first_5:.2f}%\")\n",
    "    print(f\"   Return derniers 5 √©pisodes: {last_5:.2f}%\")\n",
    "    print(f\"   Am√©lioration: {improvement:+.2f}%\")\n",
    "    \n",
    "    if improvement > 5:\n",
    "        print(\"    L'agent s'am√©liore !\")\n",
    "    elif improvement < -5:\n",
    "        print(\"    L'agent r√©gresse\")\n",
    "    else:\n",
    "        print(\"    Peu d'am√©lioration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test de l'agent entra√Æn√©\n",
    "\n",
    "Chargeons le meilleur agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur agent\n",
    "best_agent_path = Path('../models/saved/rl/best_agent.pth')\n",
    "\n",
    "# Trouver le dernier best_agent si le fichier exact n'existe pas\n",
    "if not best_agent_path.exists():\n",
    "    rl_models = list(Path('../models/saved/rl').glob('best_agent_ep*.pth'))\n",
    "    if rl_models:\n",
    "        best_agent_path = sorted(rl_models)[-1]\n",
    "        print(f\"‚úì Agent trouv√©: {best_agent_path.name}\")\n",
    "    else:\n",
    "        print(\" Aucun agent entra√Æn√© trouv√©\")\n",
    "        print(\"   Entra√Ænez d'abord un agent avec train_rl.py\")\n",
    "\n",
    "if best_agent_path.exists():\n",
    "    # Cr√©er agent\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
    "    agent.load(str(best_agent_path))\n",
    "    \n",
    "    print(f\" Agent charg√© depuis {best_agent_path}\")\n",
    "    print(f\"   √âpisodes d'entra√Ænement: {agent.episode_count}\")\n",
    "    print(f\"   Epsilon actuel: {agent.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester l'agent sur un √©pisode\n",
    "state, info = env.reset()\n",
    "actions_taken = []\n",
    "balance_history = [info['balance']]\n",
    "\n",
    "steps = 0\n",
    "max_steps = 1000  # Limiter pour le notebook\n",
    "\n",
    "while steps < max_steps:\n",
    "    # Action (greedy, pas d'exploration)\n",
    "    action = agent.select_action(state, training=False)\n",
    "    \n",
    "    # Step\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    actions_taken.append(action)\n",
    "    balance_history.append(info['balance'])\n",
    "    steps += 1\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# R√©sultats\n",
    "perf = env.get_performance_summary()\n",
    "\n",
    "print(f\"\\nüéÆ Test de l'agent ({steps} steps)\")\n",
    "print(f\"   Balance initiale: {perf['initial_balance']:.2f} ‚Ç¨\")\n",
    "print(f\"   Balance finale: {perf['final_balance']:.2f} ‚Ç¨\")\n",
    "print(f\"   Return: {perf['total_return_pct']:.2f}%\")\n",
    "print(f\"   Trades: {perf['total_trades']}\")\n",
    "print(f\"   Win Rate: {perf['win_rate']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les actions prises\n",
    "action_counts = pd.Series(actions_taken).value_counts().sort_index()\n",
    "action_names = {0: 'HOLD', 1: 'BUY', 2: 'SELL'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution des actions\n",
    "action_labels = [action_names[i] for i in action_counts.index]\n",
    "axes[0].pie(action_counts.values, labels=action_labels, autopct='%1.1f%%',\n",
    "           colors=['gray', 'green', 'red'], startangle=90)\n",
    "axes[0].set_title('Distribution des Actions', fontsize=14)\n",
    "\n",
    "# √âvolution du balance\n",
    "axes[1].plot(balance_history, linewidth=2)\n",
    "axes[1].axhline(y=perf['initial_balance'], color='black', \n",
    "               linestyle='--', alpha=0.5, label='Initial')\n",
    "axes[1].set_title('√âvolution du Balance', fontsize=14)\n",
    "axes[1].set_xlabel('Steps')\n",
    "axes[1].set_ylabel('Balance ($)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Comparaison : DQN (T08) vs ML (T07)\n",
    "\n",
    "Comparons les performances de l'agent RL avec les mod√®les ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sultats T07 (ML) - √† partir des r√©sultats pr√©c√©dents\n",
    "ml_results = {\n",
    "    'Logistic Regression': {'return': 297.54, 'trades': 10, 'win_rate': 0.40},\n",
    "    'Random Forest': {'return': 0.00, 'trades': 0, 'win_rate': 0.00},\n",
    "    'XGBoost': {'return': -0.14, 'trades': 2, 'win_rate': 0.00}\n",
    "}\n",
    "\n",
    "# R√©sultats T08 (RL)\n",
    "rl_results = {\n",
    "    'DQN Agent': {\n",
    "        'return': train_df['return'].iloc[-1],  # Dernier √©pisode\n",
    "        'trades': train_df['trades'].iloc[-1],\n",
    "        'win_rate': train_df['win_rate'].iloc[-1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Combiner\n",
    "all_results = {**ml_results, **rl_results}\n",
    "\n",
    "# Cr√©er DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "comparison_df.columns = ['Return %', 'Trades', 'Win Rate']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAISON T07 (ML) vs T08 (RL)\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "best_model = comparison_df['Return %'].idxmax()\n",
    "best_return = comparison_df['Return %'].max()\n",
    "print(f\"\\nüèÜ Meilleur mod√®le: {best_model} ({best_return:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la comparaison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Returns\n",
    "colors = ['green' if x > 0 else 'red' for x in comparison_df['Return %']]\n",
    "comparison_df['Return %'].plot(kind='bar', ax=axes[0], color=colors, alpha=0.7)\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0].set_title('Return % - Tous les mod√®les', fontsize=14)\n",
    "axes[0].set_xlabel('Mod√®le')\n",
    "axes[0].set_ylabel('Return %')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Win Rate vs Return (scatter)\n",
    "axes[1].scatter(comparison_df['Win Rate'], comparison_df['Return %'], s=200, alpha=0.6)\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    axes[1].annotate(idx, (row['Win Rate'], row['Return %']), \n",
    "                    fontsize=9, ha='center')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].axvline(x=0.5, color='black', linestyle='--', alpha=0.3)\n",
    "axes[1].set_title('Win Rate vs Return', fontsize=14)\n",
    "axes[1].set_xlabel('Win Rate')\n",
    "axes[1].set_ylabel('Return %')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "### Points cl√©s T08 :\n",
    "\n",
    "**Forces** :\n",
    "- ‚úÖ Environnement Gym custom fonctionnel\n",
    "- ‚úÖ Agent DQN avec experience replay\n",
    "- ‚úÖ Training loop complet avec validation\n",
    "- ‚úÖ Apprentissage actif (l'agent explore et apprend)\n",
    "\n",
    "**Faiblesses actuelles** :\n",
    "- ‚ùå Returns n√©gatifs (si l'agent trade trop)\n",
    "- ‚ùå Frais de transaction trop √©lev√©s\n",
    "- ‚ùå Besoin d'optimisation des hyperparam√®tres\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
