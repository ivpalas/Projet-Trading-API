{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Notebook 2 : ML Models (T07)\n",
    "\n",
    "Ce notebook explore les mod√®les de Machine Learning pour le trading GBP/USD :\n",
    "- **Feature Engineering** : Cr√©ation de features avanc√©es\n",
    "- **Target Variable** : Classification UP/DOWN/HOLD\n",
    "- **3 Mod√®les ML** : Logistic Regression, Random Forest, XGBoost\n",
    "- **Backtesting** : Performance sur donn√©es 2024\n",
    "\n",
    "**R√©sultat** : +297% de return avec Logistic Regression ! üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chargement des donn√©es avec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donn√©es avec features (depuis l'API T05)\n",
    "df_2022 = pd.read_parquet('../data/processed/m15_features_2022.parquet')\n",
    "\n",
    "print(f\" Donn√©es charg√©es (2022)\")\n",
    "print(f\"   Lignes: {len(df_2022):,}\")\n",
    "print(f\"   Colonnes: {len(df_2022.columns)}\")\n",
    "print(f\"\\nFeatures disponibles:\")\n",
    "print(list(df_2022.columns))\n",
    "\n",
    "df_2022.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Engineering (T07)\n",
    "\n",
    "Cr√©ation de features suppl√©mentaires :\n",
    "- **Lag features** : Prix/volume d√©cal√©s (t-1, t-2, t-3, t-5, t-10, t-20)\n",
    "- **Rolling statistics** : Mean, Std, Min, Max sur 5/10/20/50 p√©riodes\n",
    "- **Total** : ~100+ features cr√©√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le FeatureEngineer\n",
    "import sys\n",
    "sys.path.append('../src/models')\n",
    "from feature_engineering import FeatureEngineer, add_target_variable\n",
    "\n",
    "# Cr√©er les features ML\n",
    "engineer = FeatureEngineer()\n",
    "df_ml = engineer.create_all_features(df_2022.copy())\n",
    "\n",
    "print(f\"‚úì Features ML cr√©√©es\")\n",
    "print(f\"   Total features: {len(df_ml.columns)}\")\n",
    "print(f\"   Lignes apr√®s nettoyage: {len(df_ml):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser quelques features\n",
    "feature_columns = [col for col in df_ml.columns if col.startswith('lag_') or col.startswith('rolling_')]\n",
    "print(f\"\\n Exemples de features cr√©√©es:\")\n",
    "print(feature_columns[:10])\n",
    "\n",
    "# Stats\n",
    "df_ml[feature_columns[:5]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Target Variable\n",
    "\n",
    "Classification en 3 classes :\n",
    "- **UP (1)** : Prix augmente > 0.1% (10 pips)\n",
    "- **HOLD (0)** : Prix stable (¬±0.1%)\n",
    "- **DOWN (-1)** : Prix baisse > 0.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter la target\n",
    "df_ml = add_target_variable(df_ml, threshold=0.001, lookahead=1)\n",
    "\n",
    "# Distribution de la target\n",
    "target_dist = df_ml['target'].value_counts().sort_index()\n",
    "print(\" Distribution de la target:\")\n",
    "for label, count in target_dist.items():\n",
    "    label_name = {-1: 'DOWN', 0: 'HOLD', 1: 'UP'}[label]\n",
    "    print(f\"   {label_name}: {count:,} ({count/len(df_ml)*100:.1f}%)\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "target_dist.plot(kind='bar', ax=ax, color=['red', 'gray', 'green'])\n",
    "ax.set_title('Distribution de la Target Variable', fontsize=16)\n",
    "ax.set_xlabel('Classe')\n",
    "ax.set_ylabel('Fr√©quence')\n",
    "ax.set_xticklabels(['DOWN (-1)', 'HOLD (0)', 'UP (1)'], rotation=0)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pr√©paration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©parer features et target\n",
    "feature_cols = [col for col in df_ml.columns if col not in ['target', 'timestamp_15m']]\n",
    "X = df_ml[feature_cols]\n",
    "y = df_ml['target']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\" Donn√©es pr√©par√©es:\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Test: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Normalisation\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úì Features normalis√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Entra√Ænement des mod√®les\n",
    "\n",
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "print(\"‚úì Model trained\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_lr = model_lr.predict(X_test_scaled)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n=== Logistic Regression Performance ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['DOWN', 'HOLD', 'UP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['DOWN', 'HOLD', 'UP'],\n",
    "            yticklabels=['DOWN', 'HOLD', 'UP'])\n",
    "ax.set_title('Confusion Matrix - Logistic Regression', fontsize=14)\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "print(\"‚úì Model trained\")\n",
    "\n",
    "# Pr√©dictions\n",
    "y_pred_rf = model_rf.predict(X_test_scaled)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n=== Random Forest Performance ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['DOWN', 'HOLD', 'UP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model_rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n Top 10 Important Features:\")\n",
    "print(importances.head(10))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "importances.head(15).plot(x='feature', y='importance', kind='barh', ax=ax)\n",
    "ax.set_title('Top 15 Feature Importances - Random Forest', fontsize=14)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost (ajuster labels: -1,0,1 ‚Üí 0,1,2)\n",
    "y_train_xgb = y_train + 1\n",
    "y_test_xgb = y_test + 1\n",
    "\n",
    "model_xgb = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "model_xgb.fit(X_train_scaled, y_train_xgb)\n",
    "print(\"‚úì Model trained\")\n",
    "\n",
    "# Pr√©dictions (reconvertir 0,1,2 ‚Üí -1,0,1)\n",
    "y_pred_xgb_raw = model_xgb.predict(X_test_scaled)\n",
    "y_pred_xgb = y_pred_xgb_raw - 1\n",
    "\n",
    "# √âvaluation\n",
    "print(\"\\n=== XGBoost Performance ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['DOWN', 'HOLD', 'UP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Comparaison des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© des performances\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_xgb)\n",
    "    ],\n",
    "    'F1 (macro)': [\n",
    "        f1_score(y_test, y_pred_lr, average='macro'),\n",
    "        f1_score(y_test, y_pred_rf, average='macro'),\n",
    "        f1_score(y_test, y_pred_xgb, average='macro')\n",
    "    ],\n",
    "    'Precision (macro)': [\n",
    "        precision_score(y_test, y_pred_lr, average='macro'),\n",
    "        precision_score(y_test, y_pred_rf, average='macro'),\n",
    "        precision_score(y_test, y_pred_xgb, average='macro')\n",
    "    ],\n",
    "    'Recall (macro)': [\n",
    "        recall_score(y_test, y_pred_lr, average='macro'),\n",
    "        recall_score(y_test, y_pred_rf, average='macro'),\n",
    "        recall_score(y_test, y_pred_xgb, average='macro')\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON ===\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_model = results.loc[results['F1 (macro)'].idxmax(), 'Model']\n",
    "print(f\"\\n Best model (F1): {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Backtesting sur 2024\n",
    "\n",
    "Test des mod√®les sur donn√©es r√©elles 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les mod√®les sauvegard√©s et donn√©es 2024\n",
    "from ml_backtester import MLBacktester\n",
    "\n",
    "# Charger donn√©es 2024\n",
    "df_2024 = pd.read_parquet('../data/processed/ml_dataset_2024.parquet')\n",
    "\n",
    "print(f\" Donn√©es test charg√©es (2024)\")\n",
    "print(f\"   Lignes: {len(df_2024):,}\")\n",
    "\n",
    "# Backtester\n",
    "backtester = MLBacktester(initial_capital=10000)\n",
    "\n",
    "# Feature names\n",
    "feature_names = [col for col in df_2024.columns if col not in ['target', 'timestamp_15m']]\n",
    "\n",
    "print(\"\\n=== Backtesting sur 2024 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest Logistic Regression\n",
    "results_lr = backtester.backtest_ml_strategy(\n",
    "    df=df_2024,\n",
    "    model=model_lr,\n",
    "    scaler=scaler,\n",
    "    feature_names=feature_names,\n",
    "    model_name='logistic_regression',\n",
    "    position_size=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n=== Logistic Regression - 2024 ===\")\n",
    "print(f\"Initial Capital: {backtester.initial_capital:.2f} ‚Ç¨\")\n",
    "print(f\"Final Capital: {results_lr['final_capital']:.2f} ‚Ç¨\")\n",
    "print(f\"Total Return: {results_lr['total_return']:.2f}%\")\n",
    "print(f\"Total Trades: {results_lr['total_trades']}\")\n",
    "print(f\"Win Rate: {results_lr['win_rate']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest Random Forest\n",
    "results_rf = backtester.backtest_ml_strategy(\n",
    "    df=df_2024,\n",
    "    model=model_rf,\n",
    "    scaler=scaler,\n",
    "    feature_names=feature_names,\n",
    "    model_name='random_forest',\n",
    "    position_size=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n=== Random Forest - 2024 ===\")\n",
    "print(f\"Final Capital: {results_rf['final_capital']:.2f} ‚Ç¨\")\n",
    "print(f\"Total Return: {results_rf['total_return']:.2f}%\")\n",
    "print(f\"Total Trades: {results_rf['total_trades']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest XGBoost\n",
    "results_xgb = backtester.backtest_ml_strategy(\n",
    "    df=df_2024,\n",
    "    model=model_xgb,\n",
    "    scaler=scaler,\n",
    "    feature_names=feature_names,\n",
    "    model_name='xgboost',\n",
    "    position_size=0.95\n",
    ")\n",
    "\n",
    "print(\"\\n=== XGBoost - 2024 ===\")\n",
    "print(f\"Final Capital: {results_xgb['final_capital']:.2f} ‚Ç¨\")\n",
    "print(f\"Total Return: {results_xgb['total_return']:.2f}%\")\n",
    "print(f\"Total Trades: {results_xgb['total_trades']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Comparaison finale des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "backtest_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Return %': [\n",
    "        results_lr['total_return'],\n",
    "        results_rf['total_return'],\n",
    "        results_xgb['total_return']\n",
    "    ],\n",
    "    'Trades': [\n",
    "        results_lr['total_trades'],\n",
    "        results_rf['total_trades'],\n",
    "        results_xgb['total_trades']\n",
    "    ],\n",
    "    'Win Rate %': [\n",
    "        results_lr['win_rate'],\n",
    "        results_rf['win_rate'],\n",
    "        results_xgb['win_rate']\n",
    "    ],\n",
    "    'Final Capital': [\n",
    "        results_lr['final_capital'],\n",
    "        results_rf['final_capital'],\n",
    "        results_xgb['final_capital']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL BACKTEST COMPARISON - 2024\")\n",
    "print(\"=\" * 80)\n",
    "print(backtest_comparison.to_string(index=False))\n",
    "\n",
    "best_return = backtest_comparison.loc[backtest_comparison['Return %'].idxmax(), 'Model']\n",
    "best_return_pct = backtest_comparison['Return %'].max()\n",
    "print(f\"\\nüèÜ Best model (Return): {best_return} ({best_return_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "backtest_comparison.plot(\n",
    "    x='Model',\n",
    "    y='Return %',\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    color=['green' if x > 0 else 'red' for x in backtest_comparison['Return %']]\n",
    ")\n",
    "\n",
    "ax.set_title('Backtest Returns - 2024', fontsize=16)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Return %')\n",
    "ax.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Conclusion\n",
    "\n",
    "### R√©sultats T07 :\n",
    "\n",
    "üèÜ **Logistic Regression** : +297% de return sur 2024 !\n",
    "- Strat√©gie tr√®s s√©lective (10 trades)\n",
    "- Win rate correct (40%)\n",
    "- Performance exceptionnelle\n",
    "\n",
    "‚ö†Ô∏è **Points d'am√©lioration** :\n",
    "- D√©s√©quilibre des classes (97% HOLD)\n",
    "- Peu de signaux UP/DOWN g√©n√©r√©s\n",
    "- Besoin de class balancing (SMOTE, class_weight)\n",
    "- Optimisation des hyperparam√®tres\n",
    "\n",
    "### Prochaines √©tapes :\n",
    "- **T08** : Reinforcement Learning\n",
    "- **T09** : Production & Deployment\n",
    "- Am√©lioration T07 : Feature selection, ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
